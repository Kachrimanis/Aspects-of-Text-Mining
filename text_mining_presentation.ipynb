{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining And Analytics\n",
    "\n",
    "                                                                                        by Tasos Kachrimanis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Language Toolkit — NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The NLTK module is a massive tool kit, aimed at helping you with the entire Natural Language Processing (NLP) methodology. NLTK will aid you with everything from splitting sentences from paragraphs, splitting up words, recognizing the part of speech of those words, highlighting the main subjects, and then even with helping your machine to understand what the text is all about. In this series, we're going to tackle the field of opinion mining, or sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> **Tokenizing Words and Sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Καλησπέρα σας.', 'Θα ήθελα πληροφορίες σχετικά με το πως θα μπορούσα να ενταχθώ στο νυστερινό τιμολόγιο.']\n",
      "['Καλησπέρα', 'σας', '.', 'Θα', 'ήθελα', 'πληροφορίες', 'σχετικά', 'με', 'το', 'πως', 'θα', 'μπορούσα', 'να', 'ενταχθώ', 'στο', 'νυστερινό', 'τιμολόγιο', '.']\n",
      "Καλησπέρα\n",
      "σας\n",
      ".\n",
      "Θα\n",
      "ήθελα\n",
      "πληροφορίες\n",
      "σχετικά\n",
      "με\n",
      "το\n",
      "πως\n",
      "θα\n",
      "μπορούσα\n",
      "να\n",
      "ενταχθώ\n",
      "στο\n",
      "νυστερινό\n",
      "τιμολόγιο\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "message = \"Καλησπέρα σας. Θα ήθελα πληροφορίες σχετικά με το πως θα μπορούσα να ενταχθώ στο νυστερινό τιμολόγιο.\"\n",
    "\n",
    "print(sent_tokenize(message))\n",
    "print(word_tokenize(message))\n",
    "for i in word_tokenize(message):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Note:**\n",
    "\n",
    "Some words carry more meaning than other words and some words are just plain useless.\n",
    "We would not want these words taking up space in our database, or taking up valuable processing time. As such, we call these words \"stop words\" and we wish to do nothing with them. Another version of the term \"stop words\" can be more literal: *Words we stop on.*  For now, we'll be considering stop words as words that just contain no meaning, and we want to remove them.\n",
    "\n",
    "We can do this easily, by storing a list of words that you consider to be stop words. NLTK starts you off with a bunch of words that they consider to be stop words, you can access it via the NLTK corpus with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " 'has',\n",
       " 'hasn',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " 'she',\n",
       " 'should',\n",
       " 'shouldn',\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " 'wouldn',\n",
       " 'y',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sample',\n",
       " 'sentence',\n",
       " ',',\n",
       " 'showing',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'filtration',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> **Stemming words **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of stemming is a sort of normalizing method. Many variations of words carry the same meaning, other than when tense is involved.\n",
    "\n",
    "The reason why we stem is to shorten the lookup, and normalize sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "example_words = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> **Part of Speech Tagging** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling words in a sentence as nouns, adjectives, verbs...etc, is a powerful tool. NLTK can do this for you. The even more impressive, is that it also labels by tense, and more. Here's a list of the tags, what they mean, and some examples:\n",
    "\n",
    "\n",
    "- JJ\tadjective\t'big'\n",
    "- JJR\tadjective, comparative\t'bigger'\n",
    "- JJS\tadjective, superlative\t'biggest'\n",
    "- NN\tnoun, singular 'desk'\n",
    "- NNS\tnoun plural\t'desks'\n",
    "- NNP\tproper noun, singular\t'Harrison'\n",
    "- NNPS\tproper noun, plural\t'Americans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            \n",
    "            print(tagged)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "#Printing the Part of Speech Tagging of a sentence/text.        \n",
    "#process_content()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> **Lemmatizing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very similar operation to stemming is called lemmatizing. The major difference between these is, as you saw earlier, stemming can often create non-existent words, whereas lemmas are actual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\")) #as an adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "loving\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"loving\", pos=\"v\")) #as a verb\n",
    "print(lemmatizer.lemmatize(\"loving\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> **Wordnet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet is a lexical database for the English language, which was created by Princeton, and is part of the NLTK corpus.\n",
    "\n",
    "You can use WordNet alongside the NLTK module to find the meanings of words, synonyms, antonyms, and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "syns = wordnet.synsets(\"program\")\n",
    "\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan.n.01\n",
      "plan\n",
      "a series of steps to be carried out or goals to be accomplished\n"
     ]
    }
   ],
   "source": [
    "#synset\n",
    "print(syns[0].name())\n",
    "\n",
    "\n",
    "#just the word\n",
    "print(syns[0].lemmas()[0].name())\n",
    "\n",
    "\n",
    "#definition (for plan)\n",
    "print(syns[0].definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  How might we discern synonyms and antonyms to a word? The lemmas will be synonyms, and then you can use .antonyms to find the antonyms to the lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'skilful', 'skillful', 'goodness', 'proficient', 'honorable', 'estimable', 'practiced', 'in_force', 'well', 'full', 'commodity', 'upright', 'beneficial', 'unspoiled', 'dependable', 'adept', 'safe', 'expert', 'effective', 'right', 'thoroughly', 'salutary', 'honest', 'trade_good', 'secure', 'sound', 'respectable', 'undecomposed', 'good', 'in_effect', 'soundly', 'ripe', 'serious', 'near', 'dear', 'unspoilt', 'just'}\n",
      "{'evil', 'badness', 'evilness', 'ill', 'bad'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we can also easily use WordNet to compare the similarity of two words and their tenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n",
      "0.32\n"
     ]
    }
   ],
   "source": [
    "#compare similarity\n",
    "\n",
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"boat.n.01\")\n",
    "\n",
    "print(w1.wup_similarity(w2))\n",
    "\n",
    "\n",
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"cat.n.01\")\n",
    "\n",
    "print(w1.wup_similarity(w2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: We need to incorporate the Wu and Palmer method for semantic related-ness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Check MFQs - Most Frequent Questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Important Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this algorithm is going to be better by March\n",
      "['', 'n', 'v', 'v', '', 'v', 'a', '', 'n']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_wordnet_pos(word_tok):\n",
    "    \n",
    "        \n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "    \n",
    "word = input()\n",
    "\n",
    "word_tok = word_tokenize(word)\n",
    "pot = nltk.pos_tag(word_tok)\n",
    "\n",
    "count = 0\n",
    "pot_list = []\n",
    "for w in word_tok:\n",
    "    \n",
    "    treebank_tag = pot[count][1]\n",
    "    count += 1\n",
    "    pot_list.append(get_wordnet_pos(treebank_tag))\n",
    "\n",
    "print(pot_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
    "In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
    "- **tokenizing** strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
    "- **counting** the occurrences of tokens in each document.\n",
    "- **normalizing** and weighting with diminishing importance tokens that occur in the majority of samples / documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>Features and samples are defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each **individual token occurrence frequency** (normalized or not) is treated as a feature.\n",
    "- the vector of all the token frequencies for a given **document** is considered a multivariate **sample.**\n",
    "\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQQ9KKRdgcFD-HSDxCEgTvn5cvAlFZeIdfbcRAQq7pXQ3pWrbn7\", width=\"30%\">\n",
    "\n",
    "A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or “Bag of n-grams” representation.\n",
    "\n",
    "<img src=\"http://images.slideplayer.com/10/2811681/slides/slide_51.jpg\", width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1,  stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(2, 37)\n",
      "  (0, 26)\t0.24173784528\n",
      "  (0, 27)\t0.24173784528\n",
      "  (0, 21)\t0.171998467899\n",
      "  (0, 1)\t0.343996935798\n",
      "  (0, 0)\t0.24173784528\n",
      "  (0, 24)\t0.24173784528\n",
      "  (0, 8)\t0.171998467899\n",
      "  (0, 2)\t0.24173784528\n",
      "  (0, 36)\t0.171998467899\n",
      "  (0, 30)\t0.24173784528\n",
      "  (0, 6)\t0.24173784528\n",
      "  (0, 9)\t0.515995403697\n",
      "  (0, 5)\t0.171998467899\n",
      "  (0, 32)\t0.171998467899\n",
      "  (0, 7)\t0.24173784528\n",
      "  (1, 12)\t0.145594450865\n",
      "  (1, 33)\t0.145594450865\n",
      "  (1, 20)\t0.145594450865\n",
      "  (1, 17)\t0.145594450865\n",
      "  (1, 16)\t0.145594450865\n",
      "  (1, 15)\t0.145594450865\n",
      "  (1, 14)\t0.145594450865\n",
      "  (1, 25)\t0.436783352595\n",
      "  (1, 23)\t0.145594450865\n",
      "  (1, 18)\t0.145594450865\n",
      "  (1, 13)\t0.145594450865\n",
      "  (1, 11)\t0.145594450865\n",
      "  (1, 22)\t0.145594450865\n",
      "  (1, 4)\t0.145594450865\n",
      "  (1, 35)\t0.145594450865\n",
      "  (1, 10)\t0.145594450865\n",
      "  (1, 19)\t0.29118890173\n",
      "  (1, 34)\t0.145594450865\n",
      "  (1, 3)\t0.145594450865\n",
      "  (1, 31)\t0.145594450865\n",
      "  (1, 29)\t0.145594450865\n",
      "  (1, 28)\t0.145594450865\n",
      "  (1, 21)\t0.103591650924\n",
      "  (1, 1)\t0.103591650924\n",
      "  (1, 8)\t0.103591650924\n",
      "  (1, 36)\t0.103591650924\n",
      "  (1, 9)\t0.207183301848\n",
      "  (1, 5)\t0.207183301848\n",
      "  (1, 32)\t0.414366603696\n"
     ]
    }
   ],
   "source": [
    "content = [\"Bursting the Big Data Data bubble starts with appreciating certain nuances about Data and its products and patterns\"\n",
    "           ,\"the real solutions that are useful in dealing with certain Big Data will be needed and in demand even if the notion of Big Data falls from the height of its hype into the trough of disappointment\"]\n",
    "\n",
    "X = vectorizer.fit_transform(content)\n",
    "\n",
    "print(vectorizer)\n",
    "print(X.get_shape())\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about',\n",
       " 'and',\n",
       " 'appreciating',\n",
       " 'are',\n",
       " 'be',\n",
       " 'big',\n",
       " 'bubble',\n",
       " 'bursting',\n",
       " 'certain',\n",
       " 'data',\n",
       " 'dealing',\n",
       " 'demand',\n",
       " 'disappointment',\n",
       " 'even',\n",
       " 'falls',\n",
       " 'from',\n",
       " 'height',\n",
       " 'hype',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'its',\n",
       " 'needed',\n",
       " 'notion',\n",
       " 'nuances',\n",
       " 'of',\n",
       " 'patterns',\n",
       " 'products',\n",
       " 'real',\n",
       " 'solutions',\n",
       " 'starts',\n",
       " 'that',\n",
       " 'the',\n",
       " 'trough',\n",
       " 'useful',\n",
       " 'will',\n",
       " 'with']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 2, #features: 37\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(content)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>  **Tf–idf term weighting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf means **term-frequency** while tf–idf means term-frequency times **inverse document-frequency**:\n",
    "<img src=\"http://scikit-learn.org/stable/_images/math/40f34fb794a1d3561d64bc55e344634b1451a21f.png\", width=\"20%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "- IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Example\n",
    "\n",
    "Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'and', 'appreciating', 'are', 'be', 'big', 'bubble', 'bursting', 'certain', 'data', 'dealing', 'demand', 'disappointment', 'even', 'falls', 'from', 'height', 'hype', 'if', 'in', 'into', 'its', 'needed', 'notion', 'nuances', 'of', 'patterns', 'products', 'real', 'solutions', 'starts', 'that', 'the', 'trough', 'useful', 'will', 'with']\n"
     ]
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(content)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 26)\t0.238324028932\n",
      "  (0, 27)\t0.238324028932\n",
      "  (0, 21)\t0.169569509451\n",
      "  (0, 1)\t0.339139018902\n",
      "  (0, 0)\t0.238324028932\n",
      "  (0, 24)\t0.238324028932\n",
      "  (0, 8)\t0.238324028932\n",
      "  (0, 2)\t0.238324028932\n",
      "  (0, 36)\t0.169569509451\n",
      "  (0, 30)\t0.238324028932\n",
      "  (0, 6)\t0.238324028932\n",
      "  (0, 9)\t0.508708528353\n",
      "  (0, 5)\t0.169569509451\n",
      "  (0, 32)\t0.169569509451\n",
      "  (0, 7)\t0.238324028932\n",
      "  (1, 12)\t0.146381998863\n",
      "  (1, 33)\t0.146381998863\n",
      "  (1, 20)\t0.146381998863\n",
      "  (1, 17)\t0.146381998863\n",
      "  (1, 16)\t0.146381998863\n",
      "  (1, 15)\t0.146381998863\n",
      "  (1, 14)\t0.146381998863\n",
      "  (1, 25)\t0.439145996588\n",
      "  (1, 23)\t0.146381998863\n",
      "  (1, 18)\t0.146381998863\n",
      "  (1, 13)\t0.146381998863\n",
      "  (1, 11)\t0.146381998863\n",
      "  (1, 22)\t0.146381998863\n",
      "  (1, 4)\t0.146381998863\n",
      "  (1, 35)\t0.146381998863\n",
      "  (1, 10)\t0.146381998863\n",
      "  (1, 19)\t0.292763997726\n",
      "  (1, 34)\t0.146381998863\n",
      "  (1, 3)\t0.146381998863\n",
      "  (1, 31)\t0.146381998863\n",
      "  (1, 29)\t0.146381998863\n",
      "  (1, 28)\t0.146381998863\n",
      "  (1, 21)\t0.104151997811\n",
      "  (1, 1)\t0.104151997811\n",
      "  (1, 36)\t0.104151997811\n",
      "  (1, 9)\t0.208303995621\n",
      "  (1, 5)\t0.208303995621\n",
      "  (1, 32)\t0.416607991243\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of the Bag of Words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection of unigrams (what bag of words is) cannot capture phrases and multi-word expressions, effectively disregarding any word order dependence. Additionally, the bag of words model doesn’t account for potential misspellings or word derivations.\n",
    "Instead of building a simple collection of unigrams (n=1), one might prefer a collection of bigrams (n=2), where occurrences of pairs of consecutive words are counted.\n",
    "\n",
    "<img src=\"https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcRkBo8ElivvKC9EZhG86lgi0nMSek2l_VOIPYKA8fBLoOfH-KlD\", width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example: ['words', 'wprds']\n",
    "\n",
    "The second document contains a misspelling of the word ‘words’. A simple bag of words representation would consider these two as very distinct documents, differing in both of the two possible features. A character 2-gram representation, however, would find the documents matching in 4 out of 8 features, which may help the preferred classifier decide better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2), min_df=1)\n",
    "counts = ngram_vectorizer.fit_transform(['words', 'wprds'])\n",
    "ngram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 1, 0, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.toarray().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing a large text corpus with the hashing trick\n",
    "\n",
    "The above vectorization scheme is simple but the fact that it holds an in- memory mapping from the string tokens to the integer feature indices causes several **problems when dealing with large datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to overcome those limitations by combining the “hashing trick” (Feature hashing) implemented by the sklearn.feature_extraction.FeatureHasher class and the text preprocessing and tokenization features of the CountVectorizer.\n",
    "This combination is implementing in HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=10)\n",
    "X = hv.transform(content)\n",
    "X.get_shape()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
